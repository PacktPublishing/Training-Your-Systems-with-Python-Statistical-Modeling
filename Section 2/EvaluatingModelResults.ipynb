{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Results\n",
    "*Curtis Miller*\n",
    "\n",
    "How do we decide if a model is doing a good at predicting outcomes? We use various metrics to evaluate its performance. Again, I focus on binary classifiers. Multi-lable classifiers and regression may call for different procedures).\n",
    "\n",
    "Let's first recall our lookup algorithm, split data into training and test data, and see how it does on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic_train, titanic_test = train_test_split(titanic, test_size=0.1)\n",
    "\n",
    "def table_lookup_predictor(x, table, age):\n",
    "    \"\"\"Implements the table-lookup algorithm with ages after cufoff\"\"\"\n",
    "    \n",
    "    # Get most common label\n",
    "    default = table.Survived.value_counts().argmax()\n",
    "    # Get similar individuals\n",
    "    similar_tab = table.loc[(table[\"Pclass\"] == x[\"Pclass\"]) &\\\n",
    "                            (table[\"Sex\"] == x[\"Sex\"]) &\\\n",
    "                            (table[\"Siblings/Spouses Aboard\"] == x[\"Siblings/Spouses Aboard\"]) &\\\n",
    "                            (table[\"Parents/Children Aboard\"] == x[\"Parents/Children Aboard\"]) &\\\n",
    "                            ((table[\"Age\"] < age) == (x[\"Age\"] < age)) , \"Survived\"]\n",
    "    if len(similar_tab) == 0:\n",
    "        # If table is empty (no \"similar\" individuals), guess the most common label\n",
    "        return default\n",
    "    else:\n",
    "        return similar_tab.value_counts().argmax()\n",
    "\n",
    "actual = titanic_test.Survived\n",
    "predicted = titanic_test.apply(table_lookup_predictor, 1, table=titanic_train, age=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DataFrame({\"actual\": actual, \"predicted\": predicted})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "An obvious metric to check is the algorithm's accuracy on the test set. We've already seen how to compute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=actual, y_pred=predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy alone does not give a complete picture of how well an algorithm is doing when making predictions. It's possible that the learning problem is \"easy\". For example, if nearly everyone on the *Titanic* died, always predicting \"did not survive\" would have high accuracy, yet incorrectly predicts that every survivor died.\n",
    "\n",
    "## Precision\n",
    "\n",
    "**Precision** describes how often the model correctly predicts a given label. For example, the table lookup algorithm would have high precision for survivors if every time it predicts a passenger would be a survivor, the passenger is in fact a survivor.\n",
    "\n",
    "`precision_score()` from **sklearn** computes precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true=actual, y_pred=predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a precision score for every possible label. In particular, there is a precision score for both survivorship and death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true=actual, y_pred=predicted, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that predictions of death are more precise than prediction of survival.\n",
    "\n",
    "## Recall\n",
    "\n",
    "**Recall** is the ability of the model to correctly predict *a particular outcome*. In this example, recall is how many *Titanic* survivors were predicted by the model as being survivors. We would prefer recall to be close to 1.\n",
    "\n",
    "`recall_score()` from **sklearn** computes recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=actual, y_pred=predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only half of surviors were correctly predicted by our model to be survivors.\n",
    "\n",
    "Recall depends on the label of interest. For example, here's the recall rate for those who did not survive the *Titanic* disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true=actual, y_pred=predicted, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our algorithm does a good job at predicting deaths but a mediocre job at predicting survivors.\n",
    "\n",
    "## F1 Score\n",
    "\n",
    "The **F1 score** attempts to balance precision and recall in a single number; let $i$ be a label:\n",
    "\n",
    "$$\\text{F1}(i) = \\frac{\\text{precision}(i) \\times \\text{recall}(i)}{\\text{precision}(i) + \\text{recall}(i)}$$\n",
    "\n",
    "A score close to 1 is desirable, and a score close to 0 indicates an overall subpar model.\n",
    "\n",
    "`f1_score()` from **sklearn** computes this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=actual, y_pred=predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, depends on the label of interest\n",
    "f1_score(y_true=actual, y_pred=predicted, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the metrics mentioned so far can be computed together in a nice bundle by the **sklearn** function `classification_report()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics  import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=actual, y_pred=predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Factor\n",
    "\n",
    "A **Bayes factor** is a metric to determine which of two models better fits a dataset. We can compute a Bayes factor to determine if our algorithm is doing a better job of predicting survivorship compared to a \"dumb\" predictor that predicts the most common label. (Computing the Bayes factor can also help decide between models in the modelling phase, though we're seeing it here presumably after modelling has been completed and a predictive algorithm is nearing deployment.)\n",
    "\n",
    "Recall Bayes' Theorem:\n",
    "\n",
    "$$P(M|D) = \\frac{P(D|M)P(M)}{P(D)}$$\n",
    "\n",
    "$P(M|D)$ is roughly interpreted as the probability the model $M$ is appropriate given a dataset $D$. Let $M_1$ and $M_2$ be two competing models. The Bayes factor is then:\n",
    "\n",
    "$$K = \\frac{P(D|M_1)}{P(D|M_2)} = \\frac{P(M_1|D)}{P(M_2|D)}\\frac{P(M_2)}{P(M_1)}$$\n",
    "\n",
    "Recall that $P(M_i)$ is the prior likelihood the model $M_i$ is appropriate.\n",
    "\n",
    "Let's make this concrete. $M_1$ will denote the event that our table lookup algorithm does better than the \"naive\" algorithm, while $M_2$ is the event the \"naive\" algorithm is better than our algorithm. If $p_1$ and $p_2$ denotes the accuracy of the two algorithms, $M_1$ corresponds to $p_1 > p_2$ while $M_2$ corresponds to $p_1 < p_2$.\n",
    "\n",
    "We will use conjugate priors, assume that $p_1$ and $p_2$ are independent under the prior distribution, and both parameters follow the $\\text{Beta}(3,3)$ distribution. It can be shown that under these conditions $P(M_1) = P(M_2) = \\frac{1}{2}$ (this is not true in general); in other words, $M_1$ and $M_2$ are equally likely.\n",
    "\n",
    "We then compute the parameters of the posterior distributions of $p_1$ and $p_2$. I denote a correct prediction as a \"success\". We then need the number of \"successes\" in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(actual)    # Total sample size\n",
    "M = (actual == predicted).sum()    # A shorthand for computing the number of \"successes\n",
    "(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_params_lookup = (3 + M, 3 + N - M)\n",
    "post_params_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.Series(actual).value_counts()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_params_naive = (3 + ds[0], 3 + ds[1])\n",
    "post_params_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the simulation trick seen in a previous section to estimate $P(M_1|D) = 1 - P(M_2|D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000    # Number of simulations\n",
    "p_1 = beta.rvs(67, 28, size=N)\n",
    "p_2 = beta.rvs(64, 31, size=N)\n",
    "trial = p_1 > p_2\n",
    "\n",
    "pm1 = trial.mean()\n",
    "pm2 = 1 - pm1\n",
    "(pm1, pm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Bayes factor. Notice that $P(M_1) = P(M_2)$ so $\\frac{P(M_2)}{P(M_1)} = 1$ and $K = \\frac{P(M_1|D)}{P(M_2|D)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = pm1 / pm2\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we make of this number? We can use the Jeffreys' Scale to give meaning to $K$:\n",
    "\n",
    "|                  $K$                | Strength of Evidence |\n",
    "|-------------------------------------|----------------------|\n",
    "|                 $< 1$               |  Evidence for $M_2$  |\n",
    "|  $1$ to $10^{1/2}$ ($\\approx 3.2$)  |       Negligible     |\n",
    "|         $10^{1/2}$ to $10$          |     Substantial      |\n",
    "| $10$ to $10^{3/2}$ ($\\approx 31.6$) |        Strong        |\n",
    "|         $10^{3/2}$ to $100$         |     Very strong      |\n",
    "|              $> 100$                |      Decisive        |\n",
    "\n",
    "Our $K$ falls into the \"negligible\" range. Our algorithm seems to do barely better than the naive algorithm at predicting who survived the Titanic, but is not worth the computational effort.\n",
    "\n",
    "In the next section we will see potentially better algorithms that hopefully will have better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
