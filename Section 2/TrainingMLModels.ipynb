{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Machine Learning Models\n",
    "*Curtis Miller*\n",
    "\n",
    "In this video I discuss techniques used in machine learning.\n",
    "\n",
    "## Underfitting and Overfitting\n",
    "\n",
    "**Underfitting** is when an algorithm trained to predict a value does so poorly both in the training data and on future, unseen data.\n",
    "\n",
    "Reconsider the *Titanic* dataset example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict most common value\n",
    "if titanic.Survived.value_counts()[0] > titanic.Survived.value_counts()[1]:\n",
    "    guess = 0\n",
    "else:\n",
    "    guess = 1\n",
    "\n",
    "predicted = pd.Series([guess] * len(titanic))\n",
    "(titanic.Survived - predicted).abs().sum()    # Error count (trivial here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(titanic.Survived - predicted).abs().mean()     # Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (titanic.Survived - predicted).abs().mean()     # Correct prediction rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is underfitting as much as it possibly can. It may in fact be a worst-case-scenario for underfitting.\n",
    "\n",
    "**Overfitting** occurs when an algorithm predicts training data well but does not generalize to new data; on new data, the algorithm's error rate increases unacceptably.\n",
    "\n",
    "Underfitting is obvious when training a system, but overfitting requires more care to detect, since unseen data is not seen (obviously). There are techniques, though, for simulating unseen data.\n",
    "\n",
    "## Training / Testing Split\n",
    "\n",
    "The first technique is to split data into a training dataset and a testing dataset. We use the training data for developing our algorithm. We then see how well the algorithm generalizes by applying the trained algorithm to the test data and quantifying the error rate.\n",
    "\n",
    "`train_test_split()`, from scikit-learn (**sklearn**), makes splitting data easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_train, titanic_test = train_test_split(titanic,          # Dataset to split (array-like)\n",
    "                                               test_size=0.1)    # How large test set should be; in this case, 10% of\n",
    "                                                                 # the whole (could also be an integer for fixed size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a new algorithm. The table-lookup algorithm does the following:\n",
    "\n",
    "1. *Look up all individuals in the training set with the same passenger class (`Pclass`), sex (`Sex`), siblings/spouses aboard (`Siblings/Spouses Aboard`) and parents/children aboard (`Parents/Children Aboard`).*\n",
    "2. *Predict the most common value amongst those individuals.*\n",
    "\n",
    "Below is the code for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def table_lookup_predictor(x, table):\n",
    "    \"\"\"Implements the table-lookup algorithm\"\"\"\n",
    "    \n",
    "    # Get most common label\n",
    "    default = table.Survived.value_counts().argmax()\n",
    "    # Get similar individuals\n",
    "    similar_tab = table.loc[(table[\"Pclass\"] == x[\"Pclass\"]) &\\\n",
    "                            (table[\"Sex\"] == x[\"Sex\"]) &\\\n",
    "                            (table[\"Siblings/Spouses Aboard\"] == x[\"Siblings/Spouses Aboard\"]) &\\\n",
    "                            (table[\"Parents/Children Aboard\"] == x[\"Parents/Children Aboard\"]), \"Survived\"]\n",
    "    if len(similar_tab) == 0:\n",
    "        # If table is empty (no \"similar\" individuals), guess the most common label\n",
    "        return default\n",
    "    else:\n",
    "        return similar_tab.value_counts().argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration 1\n",
    "table_lookup_predictor(titanic_train.iloc[0,:], titanic_train)    # Perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlu_train_predicted = titanic_train.apply(table_lookup_predictor, 1,\n",
    "                                          table=titanic_train)    # Make predictions on training set\n",
    "tlu_train_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute the error our algorithm made on the training set using the scikit-learn function `accuracy_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=titanic_train.Survived,    # True values\n",
    "               y_pred=tlu_train_predicted)    # Predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is very accurate on the training set. This is to be expected; it's just looking up values from the table! What about when it's applied to the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tlu_test_predicted = titanic_test.apply(table_lookup_predictor, 1,\n",
    "                                        table=titanic_train)    # Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=titanic_test.Survived,    # True values\n",
    "               y_pred=tlu_test_predicted)    # Predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm overfit slightly on the training set, though the overfitting isn't terrible.\n",
    "\n",
    "**NOTE:** Evaluating a model on the test set should be the *very last thing you do!* If you repeatedly refer to the test set, it no longer is \"unseen\" data.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "Many algorithms include **hyperparameters**, which are parameters that are characteristic of the algorithm itself rather than the underlying phenomenon. We need to choose the value of these parameters and we are indifferent to their values beyond their ability to improve predictions.\n",
    "\n",
    "Our algorithm does not account for passengers' ages when making predictions. These unfortunately are not binary variables, but we can use them to create a binary variable by fixing an age and marking all those individuals less than this age with 1, and the rest 0. The cutoff age behaves like a hyperparameter here.\n",
    "\n",
    "We don't want to pick our cutoff to maximize predictive accuracy in the training set, though, and we don't want to choose it so that it improves accuracy in the test set either. Instead we will employ **cross-validation**. The procedure works as follows:\n",
    "\n",
    "1. *Divide data into $k$ **folds** (approximately equal size subsets of the original dataset that together form the whole dataset).*\n",
    "2. *For each fold, do the following:*\n",
    "    1. *Treat the fold as the \"test\" data and the rest of the data as \"training\" data.*\n",
    "    2. *For each possible value of the hyperparameter, use the \"training\" data to fit the model and evaluate its performance on the \"test\" data; track performance*\n",
    "3. *Aggregate the performance of the algorithm across the different folds for each possible value of the hyperparameter*\n",
    "4. *Use the hyperparameter value that overall yielded the best performance.*\n",
    "\n",
    "Cross-validation can be used for purposes other than choosing hyperparameters. For example, it can be a good way to evaluate an algorithm's performance and thus allow you to choose between different algorithms.\n",
    "\n",
    "Here I will consider six candidate cutoff ages: 10, 20, 30, 40, 50, 60. I will use 10 folds.\n",
    "\n",
    "scikit-learn provides multiple functions for supporting cross-validation. The `KFold` class can split a dataset up into folds as described. `cross_val_score()` can perform the entire cross-validation procedure, and is a good choice. Here I will do the cross-validation manually using only `KFold` but in future videos we may use `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)    # Prepare for cross-validation, creating an object for managing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preview; note that these are NumPy arrays\n",
    "for train, test in kf.split(titanic_train):\n",
    "    print(\"Training Indices:\")\n",
    "    print(train)\n",
    "    print(\"\\nTest Indices\")\n",
    "    print(test)\n",
    "    print(\"\\n----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def table_lookup_predictor_2(x, table, age):\n",
    "    \"\"\"Implements the table-lookup algorithm with ages after cufoff\"\"\"\n",
    "    \n",
    "    # Get most common label\n",
    "    default = table.Survived.value_counts().argmax()\n",
    "    # Get similar individuals\n",
    "    similar_tab = table.loc[(table[\"Pclass\"] == x[\"Pclass\"]) &\\\n",
    "                            (table[\"Sex\"] == x[\"Sex\"]) &\\\n",
    "                            (table[\"Siblings/Spouses Aboard\"] == x[\"Siblings/Spouses Aboard\"]) &\\\n",
    "                            (table[\"Parents/Children Aboard\"] == x[\"Parents/Children Aboard\"]) &\\\n",
    "                            ((table[\"Age\"] < age) == (x[\"Age\"] < age)) , \"Survived\"]\n",
    "    if len(similar_tab) == 0:\n",
    "        # If table is empty (no \"similar\" individuals), guess the most common label\n",
    "        return default\n",
    "    else:\n",
    "        return similar_tab.value_counts().argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ages = [10, 20, 30, 40, 50, 60]\n",
    "performance = dict()\n",
    "\n",
    "for age in ages:\n",
    "    cv_perf = list()\n",
    "    for train, test in kf.split(titanic_train):\n",
    "        # Get predicted values in \"test\" data using \"train\" data\n",
    "        predicted = titanic_train.iloc[test,:].apply(table_lookup_predictor_2, 1, table=titanic_train.iloc[train,:],\n",
    "                                                    age=age)\n",
    "        actual = titanic_train.loc[:,\"Survived\"].iloc[test]\n",
    "        # Add performance to a list\n",
    "        cv_perf.append(accuracy_score(y_true=actual, y_pred=predicted))\n",
    "    performance[age] = cv_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(performance).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we attain optimal performance by choosing our cutoff age to be 10 years."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
