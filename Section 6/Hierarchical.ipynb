{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "*Curtis Miller*\n",
    "\n",
    "Here I demonstrate clustering using hierarchical clustering (also known as agglomerative clustering).\n",
    "\n",
    "Hierarchical clustering starts by assigning each datapoint to a unique cluster. The algorithm then merges clusters to form bigger clusters, using some rule to decide which clusters are \"close\" to one another and thus should be merged. The algorithm repeats until there is one supercluster containing all data, but within the cluster are smaller clusters with greater granularity.\n",
    "\n",
    "Choosing the number of clusters to use amounts to how far to traverse down the hierarchical tree grouping data points together.\n",
    "\n",
    "## Aside: Similarity Scores\n",
    "\n",
    "Hierarchical clustering works by grouping \"similar\" data points together. This requires computing what are known as similarity scores. When two data points are very similar this score is high, while if two are very dissimilar this score is low.\n",
    "\n",
    "We will be computing similarity scores for the article titles in `HNHeadlines.txt` (consisting of titles of posts to [Hacker News](https://news.ycombinator.com/)). I load in that dataset as a Series below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv(\"HNHeadlines.txt\", header=None, index_col=0).iloc[:, 0]\n",
    "headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, a $n$-gram is $n$ characters that appear in sequence in a string. For example, all 3-grams of the word \"library\" are `lib`, `ibr`, `bra`, `rar`, and `ary`. These are often used to generate a feature space from a string based on its contents.\n",
    "\n",
    "I use [NLTK](http://www.nltk.org/) (Natural Language Tool Kit, a Python library for natural language processing) to form 3-grams for every headline in this dataset. (I require all words be lower-case but do not remove non-alphanumeric characters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_sets = [set(''.join(u) for u in ngrams(h.lower(), 3)) for h in headlines]\n",
    "headline_sets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have every 3-gram for every headline, let's compute the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) for each headline in the dataset. The Jaccard similarity between two sets is the number of elements the two sets have in common divided by the total number of elements in the two sets when merged together (removing duplicates). The Jaccard similarity is a number between 0 and 1, is 1 when both sets are identical, and 0 when the two sets have nothing in common. This is a common measure of similarity.\n",
    "\n",
    "Jaccard similarity is symmetric (the Jaccard similarity between sets $A$ and $B$ is the same as the Jaccard similarity between sets $B$ and $A$; order doesn't matter). We compute what's known as an affinity matrix. Each headline in our dataset will have its own row and column in the matrix, and the entry in the intersection of row $i$ and column $j$ in the matrix will be the Jaccard similarity between headline $i$ and headline $j$. Since the Jaccard similarity is symmetric, this will be the same number as the number stored in the intersection of row $j$ and column $i$ (so the affinity matrix is symmetric).\n",
    "\n",
    "Affinity matrices are necessary for hierarchical clustering. We compute the affinity matrix based on Jaccard similarity for our dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.zeros((len(headlines), len(headlines)))    # Will contain the affinity matrix\n",
    "for i in range(len(headlines)):\n",
    "    for j in range(i, len(headlines)):\n",
    "        h1, h2 = headline_sets[i], headline_sets[j]\n",
    "        js = len(h1.intersection(h2))/len(h1.union(h2))    # Compute the Jaccard similarity for the two documents\n",
    "        sims[i,j] = sims[j,i] = js    # Store the Jaccard similarity in the appropriate entries of the matrix\n",
    "\n",
    "sims[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering actually works with a distance matrix rather than a similarity matrix. (Distance matrices are similar to similarity matrices but their entries record distance rather than similarity.) It's easy to turn a matrix of Jaccard similarities into a matrix of Jaccard distances; one minus the Jaccard similarity is the Jaccard distance.\n",
    "\n",
    "## Clustering the Iris Dataset\n",
    "\n",
    "I will demonstrate using hierarchical clustering for the iris dataset. I first load in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_obj = load_iris()\n",
    "iris_data = iris_obj.data\n",
    "species = iris_obj.target\n",
    "iris_data[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iris_data[:, 0], iris_data[:, 1], c=species, cmap=plt.cm.brg)\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I import the `AgglomerativeClustering` object to perform hierarchical clustering, and then apply the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisclust1 = AgglomerativeClustering(n_clusters=3,            # Three clusters\n",
    "                                     affinity=\"euclidean\",    # \"Closeness\" is defined using Euclidean distance\n",
    "                                     linkage=\"average\")       # A cluster's location is based on the average of its member points\n",
    "irisclust1 = irisclust1.fit(iris_data)\n",
    "\n",
    "# Visualizing the clustering\n",
    "plt.scatter(iris_data[:, 0], iris_data[:, 1], c=irisclust1.labels_, cmap=plt.cm.brg)\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing different affinity and linkage schemes yields different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisclust2 = AgglomerativeClustering(n_clusters=3,            # Three clusters with cluster centers chosen as random dataset\n",
    "                                                              # points\n",
    "                                     affinity=\"manhattan\",    # \"Closeness\" is defined using Manhattan distance\n",
    "                                     linkage=\"complete\")      # A cluster's location is based on the maximum distance among\n",
    "                                                              # member points\n",
    "irisclust1 = irisclust2.fit(iris_data)\n",
    "\n",
    "# Visualizing the clustering\n",
    "plt.scatter(iris_data[:, 0], iris_data[:, 1], c=irisclust2.labels_, cmap=plt.cm.brg)\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering produces nice results but does not allow for \"prediction\". That is, it doesn't take new , never-before-seen datapoints and assign them to a cluster.\n",
    "\n",
    "## Clustering Headlines\n",
    "\n",
    "I now demonstrate clustering for the headlines dataset. Before, `AgglomerativeClustering` was generating an affinity matrix automatically using the specified distance metric. Now we provide it a precomputed affinity matrix to use. We will cluster the dataset into ten clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineclust = AgglomerativeClustering(n_clusters=10, affinity=\"precomputed\", linkage=\"average\")\n",
    "hclusters = headlineclust.fit_predict(1 - sims)    # Here we provide the affinity matrix; fit_predict() both fits and returns the\n",
    "                                               # the assigned clusters\n",
    "hclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we may want to look at metrics such as the silhouette plot to assess the quality of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "%matplotlib inline\n",
    "\n",
    "def silhouette_plot(data, labels, metric=\"euclidean\", xticks = True):\n",
    "    \"\"\"Creates a silhouette plot given a dataset and the labels corresponding to cluster assignment, and reports the\n",
    "       average silhouette score\"\"\"\n",
    "    silhouette_avg = silhouette_score(data, labels,\n",
    "                                      metric=metric)    # The average silhouette score over the entire sample\n",
    "    sample_silhouette_values = silhouette_samples(data, labels,\n",
    "                                                  metric=metric)    # The silhouette score of each individual data point\n",
    "    \n",
    "    # This loop creates the silhouettes in the silhouette plot\n",
    "    y_lower = 10    # For space between silhouettes\n",
    "    for k in np.unique(labels):\n",
    "        cluster_values = sample_silhouette_values[labels == k]\n",
    "        cluster_values.sort()\n",
    "        nk = len(cluster_values)\n",
    "        y_upper = y_lower + nk\n",
    "        color = cm.spectral(float(k) / len(np.unique(labels)))\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, cluster_values,\n",
    "                          facecolor=color, edgecolor=color)\n",
    "        plt.text(-0.05, y_lower + 0.5 * nk, str(k))\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    if xticks:\n",
    "        plt.xticks([-0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Silhouette Score\")\n",
    "    plt.ylabel(\"Cluster\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The average silhouette score is\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silhouette_plot(1 - sims, hclusters, metric=\"precomputed\", xticks=False)    # The silhouette plot requires a distance\n",
    "                                                                            # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines[hclusters == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot suggests that the clusters being produced are not particularly great. There is a megacluster that likely contains too many non-similar headlines and it should be broken up. (Negative numbers in silhouette plots, in general, are disturbing.) The good news, though, is that the clusters outside the megacluster are not necessarily bad clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
