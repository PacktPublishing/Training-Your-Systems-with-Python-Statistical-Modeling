{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "*Curtis Miller*\n",
    "\n",
    "In the Bayesian approach to regression (also referred to as Bayesian ridge regression, do to an equivalence with ridge regression), the prior distribution of the weights $\\beta$ is a Normal distribution. If the error terms $\\epsilon_i$ are assumed to be Normally distributed, the posterior distribution of the parameters is also a Normal distribution, with updated parameters. We can make predictions using the **maximum *a posteriori* (MAP)** estimates of the parameters (the values that maximize the posterior distribution's density function).\n",
    "\n",
    "**Occam's razor** refers to an empirical idea that simple models that explain phenomena are preferred to complex models that explain the same phenomena. This idea appears in Bayesian regression: the prior distribution of the parameters intentionally weights the parameters to zero. This biases the resulting linear model to be \"simple\", in that features will have negligible weights unless the features have a non-negligible predictive ability. Regression with OLS alone does not have this property; misspecified models will become as complex as necessary to overfit data.\n",
    "\n",
    "In other words, OLS can be prone to overfitting while Bayesian regression offers an approach to combat overfitting.\n",
    "\n",
    "## Choosing a Polynomial\n",
    "\n",
    "Below I load in an artificial dataset consisting of two variables, one of them the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.load(\"mystery_function.npy\")\n",
    "x, y = dat[:, 0], dat[:, 1]\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in this plot is clearly not linear but could have been generated by some other polynomial relationship. Unfortunately we don't know what polynomial relationship is appropriate and choosing the wrong one can lead to overfitting.\n",
    "\n",
    "We see this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olsfit1, olsfit2, olsfit3, olsfit4, olsfit5, olsfit6 = (LinearRegression(),)*6\n",
    "\n",
    "def gen_order_mat(x, order=1):\n",
    "    \"\"\"Generates a matrix of x useful for fitting a polynomial of some order\"\"\"\n",
    "    # Similar functionality is supplied by the vander() function in NumPy\n",
    "    \n",
    "    if order == 1:\n",
    "        return x.reshape(-1, 1)\n",
    "    else:\n",
    "        return np.array([x**i for i in range(1, order + 1)]).T\n",
    "\n",
    "# The number designates the order of the fit of the polynomial\n",
    "olsfit1 = LinearRegression().fit(gen_order_mat(x, order=1), y)\n",
    "olsfit2 = LinearRegression().fit(gen_order_mat(x, order=2), y)\n",
    "olsfit3 = LinearRegression().fit(gen_order_mat(x, order=3), y)\n",
    "olsfit4 = LinearRegression().fit(gen_order_mat(x, order=4), y)\n",
    "olsfit5 = LinearRegression().fit(gen_order_mat(x, order=5), y)\n",
    "olsfit10 = LinearRegression().fit(gen_order_mat(x, order=10), y)\n",
    "olsfit12 = LinearRegression().fit(gen_order_mat(x, order=12), y)\n",
    "\n",
    "def plotfit(fit, order=1):\n",
    "    \"\"\"Plots the function estimated by OLS.\"\"\"\n",
    "    \n",
    "    fx = np.linspace(x.min(), x.max(), num = 100)\n",
    "    fx_mat = gen_order_mat(fx, order=order)\n",
    "    yhat = fit.predict(fx_mat)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(fx, yhat)\n",
    "    plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
    "    plt.show()\n",
    "\n",
    "plotfit(olsfit1, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfit(olsfit2, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfit(olsfit3, order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfit(olsfit4, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfit(olsfit5, order=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfit(olsfit10, order=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfit(olsfit12, order=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the order of the polynomial leads to a better fit up until a certain point when new potential features lead to overfitted models. Bayesian ridge regression combats this phenomenon by biasing all parameters to 0, so when fitting a model, parameters get non-negligible contributions to the final fit only when they help in prediction.\n",
    "\n",
    "## Performing Bayesian Regression\n",
    "\n",
    "The `BayesianRidge` object allows for performing Bayesian ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesfit = BayesianRidge(alpha_1 = 1, alpha_2 = 1,\n",
    "                         lambda_1 = 30, lambda_2 = 50).fit(gen_order_mat(x, order=10), y)\n",
    "\n",
    "plotfit(bayesfit, order=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`alpha_1`, `alpha_2`, `lambda_1`, and `lambda_2` are the hyperparameters of Bayesian ridge regression as supplied by **scikit-learn** (corresponding to parameters of the prior distribution of the parameters). Changing these parameters can lead to better fits. The above function more closely resembles the \"best\" fit (the cubic curve, which was used to generate the data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
